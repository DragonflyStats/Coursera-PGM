

Probabilistic Graphical Models



Structure

Requirements



Overview

1.
The Bayesian network and Markov network representation, including extensions for reasoning over domains that change over time and over domains with a variable number of entities

2.
Reasoning and inference methods, including exact inference (variable elimination, clique trees) and approximate inference (belief propagation message passing, Markov chain Monte Carlo methods)

3.
Learning parameters and structure in PGMs

4.
Using a PGM for decision making under uncertainty.





--------------------------------------------------------------------------------


Requirements

basic concepts in discrete probability theory :Independence, conditional independence, and Bayes' rule.





--------------------------------------------------------------------------------



Structure


There will be short weekly review quizzes and programming assignments (Octave/Matlab) focusing on case studies and applications of PGMs to real-world problems:
1.
Credit scoring and insurance

2.
Genetic inheritance and disease

3.
Optical character recognition (OCR)

4.
Revisiting genetic inheritance / OCR

5.
Computer vision: Image segmentation

6.
Decision making and prenatal screening

7.
Revisiting OCR

8.
Telling humans apart from aliens with body pose data

9.
Recognizing human actions from Kinect data


First Two Weeks

To prepare for the class in advance, you may consider reading through the following sections of the textbook by Daphne and Nir Friedman:
1.
Introduction and Overview. Chapters 1, 2.1.1 - 2.1.4, 4.2.1.

2.
Bayesian Network Fundamentals. Chapters 3.1 - 3.3.

3.
Markov Network Fundamentals. Chapters 4.1, 4.2.2, 4.3.1, 4.4, 4.6.1.

4.
Structured CPDs. Chapters 5.1 - 5.5.

5.
Template Models. Chapters 6.1 - 6.4.1.





--------------------------------------------------------------------------------


Markov network


A Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph.


A Markov random field is similar to a Bayesian network in its representation of dependencies. 

It can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). 


The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model. 


A Markov random field is used to model various low- to mid-level tasks in image processing and computer vision.


For example, MRFs are used for image restoration, image completion, segmentation, texture synthesis, super-resolution and stereo matching.




--------------------------------------------------------------------------------


